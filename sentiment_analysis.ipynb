{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d061eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#!pip install transformers\n",
    "#!pip install torch\n",
    "%pip install torch==1.13.1 transformers==4.26.1 --quiet\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "DELTA_PATH = \"/mnt/nyt/archive_yearly\"  # Your previously ingested data path\n",
    "MODEL = \"assemblyai/distilbert-base-uncased-sst2\"  # Lightweight, accurate model\n",
    "BATCH_SIZE = 64                            # Adjust for memory (Databricks CE = 15GB RAM)\n",
    "\n",
    "# Load data\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.format(\"delta\").load(DELTA_PATH)\n",
    "df = df.filter(col(\"headline\").isNotNull())  # Remove nulls\n",
    "\n",
    "# Add a unique ID to every row for safe join later\n",
    "df_with_id = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "\n",
    "# 4. COLLECT HEADLINES LOCALLY FOR SENTIMENT ANALYSIS\n",
    "rows = df_with_id.select(\"row_id\", \"headline\").collect()\n",
    "headlines = [(row[\"row_id\"], row[\"headline\"]) for row in rows]\n",
    "\n",
    "\n",
    "# 5. LOAD SENTIMENT ANALYSIS PIPELINE\n",
    "# NOTE: Avoid `device_map=\"cuda\"` in CE (no GPU support)\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=MODEL, tokenizer=MODEL, truncation=True)\n",
    "\n",
    "# Run batch sentiment analysis\n",
    "results = []\n",
    "for row_id, headline in headlines:\n",
    "    try:\n",
    "        pred = sentiment_pipeline(headline)[0]\n",
    "        label = \"positive\" if pred[\"label\"] == \"LABEL_1\" else \"negative\"\n",
    "        score = float(pred[\"score\"])\n",
    "        results.append((row_id, headline, label, score))\n",
    "    except Exception as e:\n",
    "        results.append((row_id, headline, \"error\", 0.0))  # fallback on error\n",
    "        print(f\"Error processing row_id {row_id}: {e}\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 6. CREATE SPARK DATAFRAME WITH SENTIMENT\n",
    "sentiment_schema = [\"row_id\", \"headline\", \"sentiment_label\", \"sentiment_score\"]\n",
    "sentiment_df = spark.createDataFrame(results, sentiment_schema)\n",
    "sentiment_df = sentiment_df.withColumnRenamed(\"headline\", \"headline_sentiment\")\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# 7. JOIN BACK TO ORIGINAL DATAFRAME\n",
    "augmented_df = df_with_id.join(sentiment_df, on=\"row_id\", how=\"left\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 8. DISPLAY RESULTS\n",
    "augmented_df.select(\n",
    "    \"headline\", \"sentiment_label\", \"sentiment_score\", \"pub_date\"\n",
    ").orderBy(\"sentiment_score\", ascending=False).show(20, truncate=False)\n",
    "\n",
    "# --------------------------------------\n",
    "# 9. (OPTIONAL) SAVE TO DELTA TABLE\n",
    "OUTPUT_PATH = \"/mnt/nyt/sentiment_augmented\"\n",
    "augmented_df.write.format(\"delta\").mode(\"overwrite\").save(OUTPUT_PATH)\n",
    "\n",
    "# 1. Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, window,\n",
    "    collect_list, count, monotonically_increasing_id\n",
    ")\n",
    "from transformers import pipeline\n",
    "\n",
    "#\n",
    "MODEL_NAME = \"assemblyai/distilbert-base-uncased-sst2\"\n",
    "BATCH_SIZE  = 64               # tune if OOM / too slow\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# 2. Load & prepare the NYT archive\n",
    "articles = (\n",
    "    spark.table(\"nyt_archive\")                               # <- your Delta table\n",
    "         .filter(col(\"headline\").isNotNull())                \n",
    "         .withColumn(\"timestamp\", to_timestamp(col(\"pub_date\")))\n",
    "         .select(\"timestamp\", \"headline\", \"topic\")\n",
    ")\n",
    "\n",
    "# Group articles\n",
    "grouped = (\n",
    "    articles\n",
    "      .groupBy(\n",
    "          window(\"timestamp\", \"24 hours\").alias(\"time_window\"),\n",
    "          col(\"topic\")\n",
    "      )\n",
    "      .agg(\n",
    "          collect_list(\"headline\").alias(\"headlines\"),        # keep them as a list\n",
    "          count(\"*\").alias(\"article_count\")\n",
    "      )\n",
    "      .select(\n",
    "          col(\"time_window.start\").alias(\"window_start\"),\n",
    "          col(\"time_window.end\").alias(\"window_end\"),\n",
    "          \"topic\",\n",
    "          \"headlines\",\n",
    "          \"article_count\"\n",
    "      )\n",
    "      .withColumn(\"group_id\", monotonically_increasing_id())  # safe join key\n",
    ")\n",
    "\n",
    "# Run Sentiment Analysis\n",
    "groups_local = grouped.select(\"group_id\", \"headlines\").collect()\n",
    "\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=MODEL_NAME,\n",
    "    tokenizer=MODEL_NAME,\n",
    "    truncation=True,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "results = []\n",
    "for row in groups_local:\n",
    "    gid        = row[\"group_id\"]\n",
    "    headlines  = row[\"headlines\"]\n",
    "\n",
    "    # Model inference (batched internally by HF pipeline)\n",
    "    preds = sentiment_pipe(headlines)\n",
    "\n",
    "    # Convert HF labels → human-readable\n",
    "    labels = [\"positive\" if p[\"label\"] == \"LABEL_1\" else \"negative\" for p in preds]\n",
    "\n",
    "    pos_cnt = labels.count(\"positive\")\n",
    "    neg_cnt = labels.count(\"negative\")\n",
    "    maj_sent = \"positive\" if pos_cnt >= neg_cnt else \"negative\"  # tie → positive\n",
    "\n",
    "    results.append((gid, pos_cnt, neg_cnt, maj_sent))\n",
    "\n",
    "# Build a tiny DataFrame to join back\n",
    "schema = [\"group_id\", \"positive_count\", \"negative_count\", \"majority_sentiment\"]\n",
    "sentiment_df = spark.createDataFrame(results, schema)\n",
    "\n",
    "# Final dataframe\n",
    "final_df = (\n",
    "    grouped\n",
    "      .join(sentiment_df, on=\"group_id\", how=\"left\")\n",
    "      .select(\n",
    "          \"window_start\", \"window_end\", \"topic\",\n",
    "          \"article_count\", \"positive_count\", \"negative_count\",\n",
    "          \"majority_sentiment\"\n",
    "      )\n",
    "      .orderBy(\"window_start\", \"topic\")\n",
    ")\n",
    "\n",
    "\n",
    "final_df.show(truncate=False)\n",
    "\n",
    "# save as Delta \n",
    "OUTPUT_PATH = \"/mnt/nyt/sentiment_daily_topic\"\n",
    "final_df.write.format(\"delta\").mode(\"overwrite\").save(OUTPUT_PATH)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea1e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 0. (Re-)install lightweight HF model – one-time per cluster\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "%pip uninstall -y torch transformers\n",
    "%pip install --quiet torch==1.13.1 transformers==4.26.1\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Imports & helpers\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "import re\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, window, collect_list, count,\n",
    "    monotonically_increasing_id, first\n",
    ")\n",
    "\n",
    "def sanitize(name: str) -> str:\n",
    "    \"\"\"Turn an arbitrary topic into a safe column name.\"\"\"\n",
    "    return re.sub(r\"[^A-Za-z0-9]+\", \"_\", name.strip()).lower()\n",
    "\n",
    "MODEL_NAME = \"assemblyai/distilbert-base-uncased-sst2\"\n",
    "BATCH_SIZE  = 64                       # adjust for memory\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Pull the headline corpus\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "articles = (\n",
    "    spark.table(\"nyt_archive\")\n",
    "         .filter(col(\"headline\").isNotNull())\n",
    "         .withColumn(\"timestamp\", to_timestamp(col(\"pub_date\")))\n",
    "         .select(\"timestamp\", \"headline\", \"topic\")\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 3. 24-hour tumbling window × topic   (list of headlines per group)\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "groups = (\n",
    "    articles\n",
    "      .groupBy(window(\"timestamp\", \"24 hours\").alias(\"tw\"), col(\"topic\"))\n",
    "      .agg(\n",
    "          collect_list(\"headline\").alias(\"headlines\"),\n",
    "          count(\"*\").alias(\"article_count\")\n",
    "      )\n",
    "      .select(\n",
    "          col(\"tw.start\").alias(\"window_start\"),\n",
    "          col(\"tw.end\").alias(\"window_end\"),\n",
    "          \"topic\",\n",
    "          \"headlines\",\n",
    "          \"article_count\"\n",
    "      )\n",
    "      .withColumn(\"group_id\", monotonically_increasing_id())\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Driver-side sentiment inference (HF pipeline batches internally)\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "groups_local = groups.select(\"group_id\", \"headlines\").collect()\n",
    "\n",
    "sent_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=MODEL_NAME,\n",
    "    tokenizer=MODEL_NAME,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "sent_results = []\n",
    "for row in groups_local:\n",
    "    gid, hl_list = row[\"group_id\"], row[\"headlines\"]\n",
    "    preds   = sent_pipe(hl_list)\n",
    "    labels  = [\"positive\" if p[\"label\"] == \"LABEL_1\" else \"negative\" for p in preds]\n",
    "    pos, neg = labels.count(\"positive\"), labels.count(\"negative\")\n",
    "    majority = \"positive\" if pos >= neg else \"negative\"           # tie → positive\n",
    "    sent_results.append((gid, pos, neg, majority))\n",
    "\n",
    "sent_df = spark.createDataFrame(\n",
    "    sent_results,\n",
    "    [\"group_id\", \"positive_count\", \"negative_count\", \"majority_sentiment\"]\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 5. Join back – one row per window × topic\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "daily_topic_df = (\n",
    "    groups\n",
    "      .join(sent_df, on=\"group_id\", how=\"left\")\n",
    "      .select(\n",
    "          \"window_start\", \"window_end\", \"topic\",\n",
    "          \"article_count\", \"positive_count\", \"negative_count\",\n",
    "          \"majority_sentiment\"\n",
    "      )\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 6. Pivot so **each topic becomes a column**  (sentiment & counts)\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "from functools import reduce\n",
    "\n",
    "# --- Pivot majority sentiment --------------------------------------------------\n",
    "sent_pivot = (\n",
    "    daily_topic_df\n",
    "      .groupBy(\"window_start\", \"window_end\")\n",
    "      .pivot(\"topic\")\n",
    "      .agg(first(\"majority_sentiment\"))\n",
    ")\n",
    "\n",
    "# --- Pivot article counts  --------\n",
    "counts_pivot = (\n",
    "    daily_topic_df\n",
    "      .groupBy(\"window_start\", \"window_end\")\n",
    "      .pivot(\"topic\")\n",
    "      .agg(first(\"article_count\"))\n",
    ")\n",
    "\n",
    "# --- Rename columns to safe, prefixed versions ---------------------------------\n",
    "sent_safe   = sent_pivot\n",
    "counts_safe = counts_pivot\n",
    "for c in sent_pivot.columns:\n",
    "    if c not in (\"window_start\", \"window_end\"):\n",
    "        clean = sanitize(c)\n",
    "        sent_safe   = sent_safe  .withColumnRenamed(c, f\"sent_{clean}\")\n",
    "        counts_safe = counts_safe.withColumnRenamed(c, f\"count_{clean}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 7. Combine sentiment + count matrices (optional)\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "final_wide = (\n",
    "    sent_safe\n",
    "      .join(counts_safe, [\"window_start\", \"window_end\"], \"inner\")\n",
    "      .orderBy(\"window_start\")\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 8. Inspect & persist\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "display(final_wide)\n",
    "# write into Delta (append mode), partitioned\n",
    "\n",
    "DELTA_PATH  = \"/mnt/nyt/news_sentiment\"\n",
    "(\n",
    "final_wide.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(DELTA_PATH)\n",
    ")\n",
    "# 4) Register as a Hive table for easy querying\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS news_sentiment\n",
    "  USING DELTA\n",
    "  LOCATION '{DELTA_PATH}'\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Persist for downstream ML / dashboards\n",
    "# OUTPUT_PATH = \"/mnt/nyt/sentiment_daily_topic_wide\"\n",
    "# final_wide.write.format(\"delta\").mode(\"overwrite\").save(OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2384c350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# Graphs & visualizations\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# 0. CONFIG ──────────────────────────────────────────────────────\n",
    "FILE = \"/mnt/data/BDA_Sent_Analysis.xlsx\"   # adjust path if needed\n",
    "PLOT_STYLE = dict(color=\"royalblue\", alpha=0.7)\n",
    "\n",
    "# 1. LOAD & PREP ─────────────────────────────────────────────────\n",
    "df = pd.read_excel(FILE)\n",
    "\n",
    "sent_cols  = [c for c in df.columns if c.startswith(\"sent_\")]\n",
    "count_cols = [c for c in df.columns if c.startswith(\"count_\")]\n",
    "\n",
    "# helper → numeric: +1 / –1 / NaN\n",
    "def lbl(x):\n",
    "    x = str(x).lower()\n",
    "    return 1 if x == \"positive\" else -1 if x == \"negative\" else np.nan\n",
    "\n",
    "sent_num = df[sent_cols].applymap(lbl)\n",
    "\n",
    "# daily pos / neg counts (NaN ignored) + ratio\n",
    "df[\"pos_cnt\"] = (sent_num == 1).sum(axis=1)\n",
    "df[\"neg_cnt\"] = (sent_num == -1).sum(axis=1)\n",
    "df[\"pos_neg_ratio\"] = df.apply(\n",
    "    lambda r: r[\"pos_cnt\"] / r[\"neg_cnt\"] if r[\"neg_cnt\"] > 0 else np.nan,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# topic-level aggregated counts\n",
    "topic_pos = {c[5:]: (df[c].str.lower() == \"positive\").sum() for c in sent_cols}\n",
    "topic_neg = {c[5:]: (df[c].str.lower() == \"negative\").sum() for c in sent_cols}\n",
    "topic_vol = {c[6:]: df[c].fillna(0).sum() for c in count_cols}\n",
    "\n",
    "topic_ratio = {\n",
    "    k: (topic_pos[k] / topic_neg[k]) if topic_neg[k] > 0 else np.nan\n",
    "    for k in topic_pos\n",
    "}\n",
    "ratio_ser = pd.Series(topic_ratio).dropna().sort_values()\n",
    "\n",
    "# 2. SA-Fig 1 — Daily line of POS/NEG ratio ─────────────────────\n",
    "dates = pd.to_datetime(df[\"window_start\"])\n",
    "plt.figure(figsize=(12, 4.5))\n",
    "plt.plot(dates, df[\"pos_neg_ratio\"], **PLOT_STYLE, lw=2)\n",
    "plt.axhline(1, ls=\"--\", color=\"grey\", lw=0.8)\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%b-%y\"))\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Positive / Negative ratio\")\n",
    "plt.title(\"Daily positive-to-negative sentiment ratio\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"SA_Fig1_ratio_line.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 3. SA-Fig 2 — Topic bar of ratios ─────────────────────────────\n",
    "plt.figure(figsize=(9, 12))\n",
    "plt.barh(ratio_ser.index, ratio_ser.values, **PLOT_STYLE)\n",
    "plt.axvline(1, color=\"black\", lw=0.8)\n",
    "plt.xlabel(\"Positive / Negative ratio (Jan–Apr 2025)\")\n",
    "plt.title(\"Topic-level sentiment ratio (NaN where never negative)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"SA_Fig2_topic_bar.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 4. SA-Fig 3 — Volume vs ratio scatter (circle bubbles) ───────\n",
    "rows = [\n",
    "    (t, topic_vol[t], topic_ratio[t])\n",
    "    for t in topic_ratio.keys()\n",
    "    if not np.isnan(topic_ratio[t])\n",
    "]\n",
    "scat = pd.DataFrame(rows, columns=[\"topic\", \"volume\", \"ratio\"])\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(\n",
    "    scat[\"volume\"],\n",
    "    scat[\"ratio\"],\n",
    "    s=scat[\"volume\"] / 5 + 20,\n",
    "    marker=\"o\",\n",
    "    **PLOT_STYLE\n",
    ")\n",
    "for _, r in scat.nlargest(12, \"volume\").iterrows():\n",
    "    plt.text(r[\"volume\"], r[\"ratio\"] + 0.05, r[\"topic\"], fontsize=8)\n",
    "plt.axhline(1, ls=\"--\", color=\"grey\", lw=0.8)\n",
    "plt.xlabel(\"Total articles (null→0)\")\n",
    "plt.ylabel(\"Positive / Negative ratio\")\n",
    "plt.title(\"Coverage volume vs sentiment ratio by topic (Jan–Apr 2025)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"SA_Fig3_scatter.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"✓ SA-Fig1_ratio_line.png\\n✓ SA-Fig2_topic_bar.png\\n✓ SA-Fig3_scatter.png\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
