{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d061eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#!pip install transformers\n",
    "#!pip install torch\n",
    "%pip install torch==1.13.1 transformers==4.26.1 --quiet\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "DELTA_PATH = \"/mnt/nyt/archive_yearly\"  # Your previously ingested data path\n",
    "MODEL = \"assemblyai/distilbert-base-uncased-sst2\"  # Lightweight, accurate model\n",
    "BATCH_SIZE = 64                            # Adjust for memory (Databricks CE = 15GB RAM)\n",
    "\n",
    "# Load data\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.format(\"delta\").load(DELTA_PATH)\n",
    "df = df.filter(col(\"headline\").isNotNull())  # Remove nulls\n",
    "\n",
    "# Add a unique ID to every row for safe join later\n",
    "df_with_id = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "\n",
    "# 4. COLLECT HEADLINES LOCALLY FOR SENTIMENT ANALYSIS\n",
    "rows = df_with_id.select(\"row_id\", \"headline\").collect()\n",
    "headlines = [(row[\"row_id\"], row[\"headline\"]) for row in rows]\n",
    "\n",
    "\n",
    "# 5. LOAD SENTIMENT ANALYSIS PIPELINE\n",
    "# NOTE: Avoid `device_map=\"cuda\"` in CE (no GPU support)\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=MODEL, tokenizer=MODEL, truncation=True)\n",
    "\n",
    "# Run batch sentiment analysis\n",
    "results = []\n",
    "for row_id, headline in headlines:\n",
    "    try:\n",
    "        pred = sentiment_pipeline(headline)[0]\n",
    "        label = \"positive\" if pred[\"label\"] == \"LABEL_1\" else \"negative\"\n",
    "        score = float(pred[\"score\"])\n",
    "        results.append((row_id, headline, label, score))\n",
    "    except Exception as e:\n",
    "        results.append((row_id, headline, \"error\", 0.0))  # fallback on error\n",
    "        print(f\"Error processing row_id {row_id}: {e}\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 6. CREATE SPARK DATAFRAME WITH SENTIMENT\n",
    "sentiment_schema = [\"row_id\", \"headline\", \"sentiment_label\", \"sentiment_score\"]\n",
    "sentiment_df = spark.createDataFrame(results, sentiment_schema)\n",
    "sentiment_df = sentiment_df.withColumnRenamed(\"headline\", \"headline_sentiment\")\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# 7. JOIN BACK TO ORIGINAL DATAFRAME\n",
    "augmented_df = df_with_id.join(sentiment_df, on=\"row_id\", how=\"left\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 8. DISPLAY RESULTS\n",
    "augmented_df.select(\n",
    "    \"headline\", \"sentiment_label\", \"sentiment_score\", \"pub_date\"\n",
    ").orderBy(\"sentiment_score\", ascending=False).show(20, truncate=False)\n",
    "\n",
    "# --------------------------------------\n",
    "# 9. (OPTIONAL) SAVE TO DELTA TABLE\n",
    "OUTPUT_PATH = \"/mnt/nyt/sentiment_augmented\"\n",
    "augmented_df.write.format(\"delta\").mode(\"overwrite\").save(OUTPUT_PATH)\n",
    "\n",
    "# 1. Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, window,\n",
    "    collect_list, count, monotonically_increasing_id\n",
    ")\n",
    "from transformers import pipeline\n",
    "\n",
    "#\n",
    "MODEL_NAME = \"assemblyai/distilbert-base-uncased-sst2\"\n",
    "BATCH_SIZE  = 64               # tune if OOM / too slow\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# 2. Load & prepare the NYT archive\n",
    "articles = (\n",
    "    spark.table(\"nyt_archive\")                               # <- your Delta table\n",
    "         .filter(col(\"headline\").isNotNull())                \n",
    "         .withColumn(\"timestamp\", to_timestamp(col(\"pub_date\")))\n",
    "         .select(\"timestamp\", \"headline\", \"topic\")\n",
    ")\n",
    "\n",
    "# Group articles\n",
    "grouped = (\n",
    "    articles\n",
    "      .groupBy(\n",
    "          window(\"timestamp\", \"24 hours\").alias(\"time_window\"),\n",
    "          col(\"topic\")\n",
    "      )\n",
    "      .agg(\n",
    "          collect_list(\"headline\").alias(\"headlines\"),        # keep them as a list\n",
    "          count(\"*\").alias(\"article_count\")\n",
    "      )\n",
    "      .select(\n",
    "          col(\"time_window.start\").alias(\"window_start\"),\n",
    "          col(\"time_window.end\").alias(\"window_end\"),\n",
    "          \"topic\",\n",
    "          \"headlines\",\n",
    "          \"article_count\"\n",
    "      )\n",
    "      .withColumn(\"group_id\", monotonically_increasing_id())  # safe join key\n",
    ")\n",
    "\n",
    "# Run Sentiment Analysis\n",
    "groups_local = grouped.select(\"group_id\", \"headlines\").collect()\n",
    "\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=MODEL_NAME,\n",
    "    tokenizer=MODEL_NAME,\n",
    "    truncation=True,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "results = []\n",
    "for row in groups_local:\n",
    "    gid        = row[\"group_id\"]\n",
    "    headlines  = row[\"headlines\"]\n",
    "\n",
    "    # Model inference (batched internally by HF pipeline)\n",
    "    preds = sentiment_pipe(headlines)\n",
    "\n",
    "    # Convert HF labels → human-readable\n",
    "    labels = [\"positive\" if p[\"label\"] == \"LABEL_1\" else \"negative\" for p in preds]\n",
    "\n",
    "    pos_cnt = labels.count(\"positive\")\n",
    "    neg_cnt = labels.count(\"negative\")\n",
    "    maj_sent = \"positive\" if pos_cnt >= neg_cnt else \"negative\"  # tie → positive\n",
    "\n",
    "    results.append((gid, pos_cnt, neg_cnt, maj_sent))\n",
    "\n",
    "# Build a tiny DataFrame to join back\n",
    "schema = [\"group_id\", \"positive_count\", \"negative_count\", \"majority_sentiment\"]\n",
    "sentiment_df = spark.createDataFrame(results, schema)\n",
    "\n",
    "# Final dataframe\n",
    "final_df = (\n",
    "    grouped\n",
    "      .join(sentiment_df, on=\"group_id\", how=\"left\")\n",
    "      .select(\n",
    "          \"window_start\", \"window_end\", \"topic\",\n",
    "          \"article_count\", \"positive_count\", \"negative_count\",\n",
    "          \"majority_sentiment\"\n",
    "      )\n",
    "      .orderBy(\"window_start\", \"topic\")\n",
    ")\n",
    "\n",
    "\n",
    "final_df.show(truncate=False)\n",
    "\n",
    "# save as Delta \n",
    "OUTPUT_PATH = \"/mnt/nyt/sentiment_daily_topic\"\n",
    "final_df.write.format(\"delta\").mode(\"overwrite\").save(OUTPUT_PATH)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea1e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 0. (Re-)install lightweight HF model – one-time per cluster\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "%pip uninstall -y torch transformers\n",
    "%pip install --quiet torch==1.13.1 transformers==4.26.1\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Imports & helpers\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "import re\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, window, collect_list, count,\n",
    "    monotonically_increasing_id, first\n",
    ")\n",
    "\n",
    "def sanitize(name: str) -> str:\n",
    "    \"\"\"Turn an arbitrary topic into a safe column name.\"\"\"\n",
    "    return re.sub(r\"[^A-Za-z0-9]+\", \"_\", name.strip()).lower()\n",
    "\n",
    "MODEL_NAME = \"assemblyai/distilbert-base-uncased-sst2\"\n",
    "BATCH_SIZE  = 64                       # adjust for memory\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Pull the headline corpus\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "articles = (\n",
    "    spark.table(\"nyt_archive\")\n",
    "         .filter(col(\"headline\").isNotNull())\n",
    "         .withColumn(\"timestamp\", to_timestamp(col(\"pub_date\")))\n",
    "         .select(\"timestamp\", \"headline\", \"topic\")\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 3. 24-hour tumbling window × topic   (list of headlines per group)\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "groups = (\n",
    "    articles\n",
    "      .groupBy(window(\"timestamp\", \"24 hours\").alias(\"tw\"), col(\"topic\"))\n",
    "      .agg(\n",
    "          collect_list(\"headline\").alias(\"headlines\"),\n",
    "          count(\"*\").alias(\"article_count\")\n",
    "      )\n",
    "      .select(\n",
    "          col(\"tw.start\").alias(\"window_start\"),\n",
    "          col(\"tw.end\").alias(\"window_end\"),\n",
    "          \"topic\",\n",
    "          \"headlines\",\n",
    "          \"article_count\"\n",
    "      )\n",
    "      .withColumn(\"group_id\", monotonically_increasing_id())\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Driver-side sentiment inference (HF pipeline batches internally)\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "groups_local = groups.select(\"group_id\", \"headlines\").collect()\n",
    "\n",
    "sent_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=MODEL_NAME,\n",
    "    tokenizer=MODEL_NAME,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "sent_results = []\n",
    "for row in groups_local:\n",
    "    gid, hl_list = row[\"group_id\"], row[\"headlines\"]\n",
    "    preds   = sent_pipe(hl_list)\n",
    "    labels  = [\"positive\" if p[\"label\"] == \"LABEL_1\" else \"negative\" for p in preds]\n",
    "    pos, neg = labels.count(\"positive\"), labels.count(\"negative\")\n",
    "    majority = \"positive\" if pos >= neg else \"negative\"           # tie → positive\n",
    "    sent_results.append((gid, pos, neg, majority))\n",
    "\n",
    "sent_df = spark.createDataFrame(\n",
    "    sent_results,\n",
    "    [\"group_id\", \"positive_count\", \"negative_count\", \"majority_sentiment\"]\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 5. Join back – one row per window × topic\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "daily_topic_df = (\n",
    "    groups\n",
    "      .join(sent_df, on=\"group_id\", how=\"left\")\n",
    "      .select(\n",
    "          \"window_start\", \"window_end\", \"topic\",\n",
    "          \"article_count\", \"positive_count\", \"negative_count\",\n",
    "          \"majority_sentiment\"\n",
    "      )\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 6. Pivot so **each topic becomes a column**  (sentiment & counts)\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "from functools import reduce\n",
    "\n",
    "# --- Pivot majority sentiment --------------------------------------------------\n",
    "sent_pivot = (\n",
    "    daily_topic_df\n",
    "      .groupBy(\"window_start\", \"window_end\")\n",
    "      .pivot(\"topic\")\n",
    "      .agg(first(\"majority_sentiment\"))\n",
    ")\n",
    "\n",
    "# --- Pivot article counts  --------\n",
    "counts_pivot = (\n",
    "    daily_topic_df\n",
    "      .groupBy(\"window_start\", \"window_end\")\n",
    "      .pivot(\"topic\")\n",
    "      .agg(first(\"article_count\"))\n",
    ")\n",
    "\n",
    "# --- Rename columns to safe, prefixed versions ---------------------------------\n",
    "sent_safe   = sent_pivot\n",
    "counts_safe = counts_pivot\n",
    "for c in sent_pivot.columns:\n",
    "    if c not in (\"window_start\", \"window_end\"):\n",
    "        clean = sanitize(c)\n",
    "        sent_safe   = sent_safe  .withColumnRenamed(c, f\"sent_{clean}\")\n",
    "        counts_safe = counts_safe.withColumnRenamed(c, f\"count_{clean}\")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 7. Combine sentiment + count matrices (optional)\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "final_wide = (\n",
    "    sent_safe\n",
    "      .join(counts_safe, [\"window_start\", \"window_end\"], \"inner\")\n",
    "      .orderBy(\"window_start\")\n",
    ")\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# 8. Inspect & persist\n",
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "display(final_wide)\n",
    "# write into Delta (append mode), partitioned\n",
    "\n",
    "DELTA_PATH  = \"/mnt/nyt/news_sentiment\"\n",
    "(\n",
    "final_wide.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save(DELTA_PATH)\n",
    ")\n",
    "# 4) Register as a Hive table for easy querying\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE TABLE IF NOT EXISTS news_sentiment\n",
    "  USING DELTA\n",
    "  LOCATION '{DELTA_PATH}'\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Persist for downstream ML / dashboards\n",
    "# OUTPUT_PATH = \"/mnt/nyt/sentiment_daily_topic_wide\"\n",
    "# final_wide.write.format(\"delta\").mode(\"overwrite\").save(OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2384c350",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/BDA_Sent_Analysis.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m PLOT_STYLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroyalblue\u001b[39m\u001b[38;5;124m\"\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 1. LOAD & PREP ─────────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m sent_cols  \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msent_\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     17\u001b[0m count_cols \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount_\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "File \u001b[1;32mc:\\Users\\sacar\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\sacar\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\sacar\\anaconda3\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sacar\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/BDA_Sent_Analysis.xlsx'"
     ]
    }
   ],
   "source": [
    "# ────────────────────────────────────────────────────────────────\n",
    "# Graphs & visualizations\n",
    "# ────────────────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# 0. CONFIG ──────────────────────────────────────────────────────\n",
    "FILE = \"/mnt/data/BDA_Sent_Analysis.xlsx\"   # adjust path if needed\n",
    "PLOT_STYLE = dict(color=\"royalblue\", alpha=0.7)\n",
    "\n",
    "'''\n",
    "# 1. LOAD & PREP ─────────────────────────────────────────────────\n",
    "df = pd.read_excel(FILE)\n",
    "\n",
    "sent_cols  = [c for c in df.columns if c.startswith(\"sent_\")]\n",
    "count_cols = [c for c in df.columns if c.startswith(\"count_\")]\n",
    "\n",
    "# helper → numeric: +1 / –1 / NaN\n",
    "def lbl(x):\n",
    "    x = str(x).lower()\n",
    "    return 1 if x == \"positive\" else -1 if x == \"negative\" else np.nan\n",
    "\n",
    "sent_num = df[sent_cols].applymap(lbl)\n",
    "\n",
    "# daily pos / neg counts (NaN ignored) + ratio\n",
    "df[\"pos_cnt\"] = (sent_num == 1).sum(axis=1)\n",
    "df[\"neg_cnt\"] = (sent_num == -1).sum(axis=1)\n",
    "df[\"pos_neg_ratio\"] = df.apply(\n",
    "    lambda r: r[\"pos_cnt\"] / r[\"neg_cnt\"] if r[\"neg_cnt\"] > 0 else np.nan,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# topic-level aggregated counts\n",
    "topic_pos = {c[5:]: (df[c].str.lower() == \"positive\").sum() for c in sent_cols}\n",
    "topic_neg = {c[5:]: (df[c].str.lower() == \"negative\").sum() for c in sent_cols}\n",
    "topic_vol = {c[6:]: df[c].fillna(0).sum() for c in count_cols}\n",
    "\n",
    "topic_ratio = {\n",
    "    k: (topic_pos[k] / topic_neg[k]) if topic_neg[k] > 0 else np.nan\n",
    "    for k in topic_pos\n",
    "}\n",
    "ratio_ser = pd.Series(topic_ratio).dropna().sort_values()\n",
    "'''\n",
    "\n",
    "df   = pd.read_excel(FILE)\n",
    "\n",
    "sent_cols = [c for c in df.columns if c.startswith(\"sent_\")]\n",
    "\n",
    "# ─── 2. Map headline labels → +1 / –1 / NaN ----------------------------------\n",
    "def to_num(x):\n",
    "    x = str(x).lower()\n",
    "    if x == \"positive\":  return  1\n",
    "    if x == \"negative\":  return -1\n",
    "    return np.nan                        # null or neutral\n",
    "\n",
    "sent_num = df[sent_cols].applymap(to_num)\n",
    "\n",
    "# daily counts (NaN ignored)\n",
    "df[\"pos_cnt\"] = (sent_num == 1).sum(axis=1)\n",
    "df[\"neg_cnt\"] = (sent_num == -1).sum(axis=1)\n",
    "\n",
    "# ─── 3. Plot ------------------------------------------------------------------\n",
    "dates = pd.to_datetime(df[\"window_start\"])\n",
    "\n",
    "plt.figure(figsize=(12, 4.8))\n",
    "plt.plot(dates, df[\"pos_cnt\"], label=\"Positive\",  color=\"royalblue\", lw=2)\n",
    "plt.plot(dates, df[\"neg_cnt\"], label=\"Negative\", color=\"lightblue\", lw=2)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%b-%y\"))\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"# topic labels (≠ NaN)\")\n",
    "plt.title(\"Daily count of positive vs negative topic labels\")\n",
    "plt.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"SA_Fig_counts_line.png\", dpi=300)  # optional\n",
    "plt.show()\n",
    "# 2. SA-Fig 1 — Daily line of POS/NEG ratio ─────────────────────\n",
    "dates = pd.to_datetime(df[\"window_start\"])\n",
    "plt.figure(figsize=(12, 4.5))\n",
    "plt.plot(dates, df[\"pos_neg_ratio\"], **PLOT_STYLE, lw=2)\n",
    "plt.axhline(1, ls=\"--\", color=\"grey\", lw=0.8)\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%b-%y\"))\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Positive / Negative ratio\")\n",
    "plt.title(\"Daily positive-to-negative sentiment ratio\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"SA_Fig1_ratio_line.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 3. SA-Fig 2 — Topic bar of ratios ─────────────────────────────\n",
    "plt.figure(figsize=(9, 12))\n",
    "plt.barh(ratio_ser.index, ratio_ser.values, **PLOT_STYLE)\n",
    "plt.axvline(1, color=\"black\", lw=0.8)\n",
    "plt.xlabel(\"Positive / Negative ratio (Jan–Apr 2025)\")\n",
    "plt.title(\"Topic-level sentiment ratio (NaN where never negative)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"SA_Fig2_topic_bar.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 4. SA-Fig 3 — Volume vs ratio scatter (circle bubbles) ───────\n",
    "rows = [\n",
    "    (t, topic_vol[t], topic_ratio[t])\n",
    "    for t in topic_ratio.keys()\n",
    "    if not np.isnan(topic_ratio[t])\n",
    "]\n",
    "scat = pd.DataFrame(rows, columns=[\"topic\", \"volume\", \"ratio\"])\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(\n",
    "    scat[\"volume\"],\n",
    "    scat[\"ratio\"],\n",
    "    s=scat[\"volume\"] / 5 + 20,\n",
    "    marker=\"o\",\n",
    "    **PLOT_STYLE\n",
    ")\n",
    "for _, r in scat.nlargest(12, \"volume\").iterrows():\n",
    "    plt.text(r[\"volume\"], r[\"ratio\"] + 0.05, r[\"topic\"], fontsize=8)\n",
    "plt.axhline(1, ls=\"--\", color=\"grey\", lw=0.8)\n",
    "plt.xlabel(\"Total articles (null→0)\")\n",
    "plt.ylabel(\"Positive / Negative ratio\")\n",
    "plt.title(\"Coverage volume vs sentiment ratio by topic (Jan–Apr 2025)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"SA_Fig3_scatter.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"✓ SA-Fig1_ratio_line.png\\n✓ SA-Fig2_topic_bar.png\\n✓ SA-Fig3_scatter.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
