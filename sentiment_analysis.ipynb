{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d061eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "#!pip install torch\n",
    "%pip install torch==1.13.1 transformers==4.26.1 --quiet\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "DELTA_PATH = \"/mnt/nyt/archive_yearly\"  # Your previously ingested data path\n",
    "MODEL = \"assemblyai/distilbert-base-uncased-sst2\"  # Lightweight, accurate model\n",
    "BATCH_SIZE = 64                            # Adjust for memory (Databricks CE = 15GB RAM)\n",
    "\n",
    "# Load data\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.format(\"delta\").load(DELTA_PATH)\n",
    "df = df.filter(col(\"headline\").isNotNull())  # Remove nulls\n",
    "\n",
    "# Add a unique ID to every row for safe join later\n",
    "df_with_id = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "\n",
    "# 4. COLLECT HEADLINES LOCALLY FOR SENTIMENT ANALYSIS\n",
    "rows = df_with_id.select(\"row_id\", \"headline\").collect()\n",
    "headlines = [(row[\"row_id\"], row[\"headline\"]) for row in rows]\n",
    "\n",
    "\n",
    "# 5. LOAD SENTIMENT ANALYSIS PIPELINE\n",
    "# NOTE: Avoid `device_map=\"cuda\"` in CE (no GPU support)\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=MODEL, tokenizer=MODEL, truncation=True)\n",
    "\n",
    "# Run batch sentiment analysis\n",
    "results = []\n",
    "for row_id, headline in headlines:\n",
    "    try:\n",
    "        pred = sentiment_pipeline(headline)[0]\n",
    "        label = \"positive\" if pred[\"label\"] == \"LABEL_1\" else \"negative\"\n",
    "        score = float(pred[\"score\"])\n",
    "        results.append((row_id, headline, label, score))\n",
    "    except Exception as e:\n",
    "        results.append((row_id, headline, \"error\", 0.0))  # fallback on error\n",
    "        print(f\"Error processing row_id {row_id}: {e}\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 6. CREATE SPARK DATAFRAME WITH SENTIMENT\n",
    "sentiment_schema = [\"row_id\", \"headline\", \"sentiment_label\", \"sentiment_score\"]\n",
    "sentiment_df = spark.createDataFrame(results, sentiment_schema)\n",
    "sentiment_df = sentiment_df.withColumnRenamed(\"headline\", \"headline_sentiment\")\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# 7. JOIN BACK TO ORIGINAL DATAFRAME\n",
    "augmented_df = df_with_id.join(sentiment_df, on=\"row_id\", how=\"left\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 8. DISPLAY RESULTS\n",
    "augmented_df.select(\n",
    "    \"headline\", \"sentiment_label\", \"sentiment_score\", \"pub_date\"\n",
    ").orderBy(\"sentiment_score\", ascending=False).show(20, truncate=False)\n",
    "\n",
    "# --------------------------------------\n",
    "# 9. (OPTIONAL) SAVE TO DELTA TABLE\n",
    "OUTPUT_PATH = \"/mnt/nyt/sentiment_augmented\"\n",
    "augmented_df.write.format(\"delta\").mode(\"overwrite\").save(OUTPUT_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
