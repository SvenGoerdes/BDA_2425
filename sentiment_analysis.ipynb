{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d061eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#!pip install transformers\n",
    "#!pip install torch\n",
    "%pip install torch==1.13.1 transformers==4.26.1 --quiet\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "DELTA_PATH = \"/mnt/nyt/archive_yearly\"  # Your previously ingested data path\n",
    "MODEL = \"assemblyai/distilbert-base-uncased-sst2\"  # Lightweight, accurate model\n",
    "BATCH_SIZE = 64                            # Adjust for memory (Databricks CE = 15GB RAM)\n",
    "\n",
    "# Load data\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.format(\"delta\").load(DELTA_PATH)\n",
    "df = df.filter(col(\"headline\").isNotNull())  # Remove nulls\n",
    "\n",
    "# Add a unique ID to every row for safe join later\n",
    "df_with_id = df.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "\n",
    "# 4. COLLECT HEADLINES LOCALLY FOR SENTIMENT ANALYSIS\n",
    "rows = df_with_id.select(\"row_id\", \"headline\").collect()\n",
    "headlines = [(row[\"row_id\"], row[\"headline\"]) for row in rows]\n",
    "\n",
    "\n",
    "# 5. LOAD SENTIMENT ANALYSIS PIPELINE\n",
    "# NOTE: Avoid `device_map=\"cuda\"` in CE (no GPU support)\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=MODEL, tokenizer=MODEL, truncation=True)\n",
    "\n",
    "# Run batch sentiment analysis\n",
    "results = []\n",
    "for row_id, headline in headlines:\n",
    "    try:\n",
    "        pred = sentiment_pipeline(headline)[0]\n",
    "        label = \"positive\" if pred[\"label\"] == \"LABEL_1\" else \"negative\"\n",
    "        score = float(pred[\"score\"])\n",
    "        results.append((row_id, headline, label, score))\n",
    "    except Exception as e:\n",
    "        results.append((row_id, headline, \"error\", 0.0))  # fallback on error\n",
    "        print(f\"Error processing row_id {row_id}: {e}\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 6. CREATE SPARK DATAFRAME WITH SENTIMENT\n",
    "sentiment_schema = [\"row_id\", \"headline\", \"sentiment_label\", \"sentiment_score\"]\n",
    "sentiment_df = spark.createDataFrame(results, sentiment_schema)\n",
    "sentiment_df = sentiment_df.withColumnRenamed(\"headline\", \"headline_sentiment\")\n",
    "\n",
    "\n",
    "# --------------------------------------\n",
    "# 7. JOIN BACK TO ORIGINAL DATAFRAME\n",
    "augmented_df = df_with_id.join(sentiment_df, on=\"row_id\", how=\"left\")\n",
    "\n",
    "# --------------------------------------\n",
    "# 8. DISPLAY RESULTS\n",
    "augmented_df.select(\n",
    "    \"headline\", \"sentiment_label\", \"sentiment_score\", \"pub_date\"\n",
    ").orderBy(\"sentiment_score\", ascending=False).show(20, truncate=False)\n",
    "\n",
    "# --------------------------------------\n",
    "# 9. (OPTIONAL) SAVE TO DELTA TABLE\n",
    "OUTPUT_PATH = \"/mnt/nyt/sentiment_augmented\"\n",
    "augmented_df.write.format(\"delta\").mode(\"overwrite\").save(OUTPUT_PATH)\n",
    "'''\n",
    "\n",
    "\n",
    "# 1. Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, window,\n",
    "    collect_list, count, monotonically_increasing_id\n",
    ")\n",
    "from transformers import pipeline\n",
    "\n",
    "#\n",
    "MODEL_NAME = \"assemblyai/distilbert-base-uncased-sst2\"\n",
    "BATCH_SIZE  = 64               # tune if OOM / too slow\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# 2. Load & prepare the NYT archive\n",
    "articles = (\n",
    "    spark.table(\"nyt_archive\")                               # <- your Delta table\n",
    "         .filter(col(\"headline\").isNotNull())                \n",
    "         .withColumn(\"timestamp\", to_timestamp(col(\"pub_date\")))\n",
    "         .select(\"timestamp\", \"headline\", \"topic\")\n",
    ")\n",
    "\n",
    "# Group articles\n",
    "grouped = (\n",
    "    articles\n",
    "      .groupBy(\n",
    "          window(\"timestamp\", \"24 hours\").alias(\"time_window\"),\n",
    "          col(\"topic\")\n",
    "      )\n",
    "      .agg(\n",
    "          collect_list(\"headline\").alias(\"headlines\"),        # keep them as a list\n",
    "          count(\"*\").alias(\"article_count\")\n",
    "      )\n",
    "      .select(\n",
    "          col(\"time_window.start\").alias(\"window_start\"),\n",
    "          col(\"time_window.end\").alias(\"window_end\"),\n",
    "          \"topic\",\n",
    "          \"headlines\",\n",
    "          \"article_count\"\n",
    "      )\n",
    "      .withColumn(\"group_id\", monotonically_increasing_id())  # safe join key\n",
    ")\n",
    "\n",
    "# Run Sentiment Analysis\n",
    "groups_local = grouped.select(\"group_id\", \"headlines\").collect()\n",
    "\n",
    "sentiment_pipe = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=MODEL_NAME,\n",
    "    tokenizer=MODEL_NAME,\n",
    "    truncation=True,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "results = []\n",
    "for row in groups_local:\n",
    "    gid        = row[\"group_id\"]\n",
    "    headlines  = row[\"headlines\"]\n",
    "\n",
    "    # Model inference (batched internally by HF pipeline)\n",
    "    preds = sentiment_pipe(headlines)\n",
    "\n",
    "    # Convert HF labels → human-readable\n",
    "    labels = [\"positive\" if p[\"label\"] == \"LABEL_1\" else \"negative\" for p in preds]\n",
    "\n",
    "    pos_cnt = labels.count(\"positive\")\n",
    "    neg_cnt = labels.count(\"negative\")\n",
    "    maj_sent = \"positive\" if pos_cnt >= neg_cnt else \"negative\"  # tie → positive\n",
    "\n",
    "    results.append((gid, pos_cnt, neg_cnt, maj_sent))\n",
    "\n",
    "# Build a tiny DataFrame to join back\n",
    "schema = [\"group_id\", \"positive_count\", \"negative_count\", \"majority_sentiment\"]\n",
    "sentiment_df = spark.createDataFrame(results, schema)\n",
    "\n",
    "# Final dataframe\n",
    "final_df = (\n",
    "    grouped\n",
    "      .join(sentiment_df, on=\"group_id\", how=\"left\")\n",
    "      .select(\n",
    "          \"window_start\", \"window_end\", \"topic\",\n",
    "          \"article_count\", \"positive_count\", \"negative_count\",\n",
    "          \"majority_sentiment\"\n",
    "      )\n",
    "      .orderBy(\"window_start\", \"topic\")\n",
    ")\n",
    "\n",
    "\n",
    "final_df.show(truncate=False)\n",
    "\n",
    "# save as Delta \n",
    "OUTPUT_PATH = \"/mnt/nyt/sentiment_daily_topic\"\n",
    "final_df.write.format(\"delta\").mode(\"overwrite\").save(OUTPUT_PATH)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
