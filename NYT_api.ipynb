{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00f4af3d",
   "metadata": {},
   "source": [
    "Pulls from the New York Times all Headlines from 2024. Can be refined using special topics as key or different time frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a0c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY     = \"9cLqd9jAufochxZTdf3XW0MVh4mvzGIO\"\n",
    "BASE_URL    = \"https://api.nytimes.com/svc/archive/v1\"\n",
    "DELTA_PATH  = \"/mnt/nyt/archive_yearly\"\n",
    "TARGET_YEAR = 2024\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, dayofmonth, hour\n",
    "\n",
    "# 1) Initialize Spark (on Databricks this is implicit)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# 2) Loop over each month of the target year\n",
    "for m in range(1, 13):\n",
    "    # Construct the URL for that month’s archive\n",
    "    url = f\"{BASE_URL}/{TARGET_YEAR}/{m}.json\"\n",
    "    params = {\"api-key\": API_KEY}\n",
    "    \n",
    "    # 3) Fetch the monthly JSON (all articles for year/month) \n",
    "    resp = requests.get(url, params=params)\n",
    "    resp.raise_for_status()  # No rate limits—will not 429\n",
    "    month_json = resp.json()\n",
    "    \n",
    "    # 4) Extract the docs array (schema matches Article Search API) \n",
    "    docs = month_json.get(\"response\", {}).get(\"docs\", [])\n",
    "    if not docs:\n",
    "        continue  # Skip empty months\n",
    "    \n",
    "    # 5) Parallelize and read JSON into a DataFrame\n",
    "    rdd = spark.sparkContext.parallelize([json.dumps(d) for d in docs])\n",
    "    df  = spark.read.json(rdd)\n",
    "    \n",
    "    # 6) Select & rename desired fields\n",
    "    df_sel = df.select(\n",
    "        col(\"headline.main\").alias(\"headline\"),\n",
    "        col(\"pub_date\").alias(\"pub_date\"),\n",
    "        col(\"web_url\"),\n",
    "        col(\"section_name\"),\n",
    "        col(\"news_desk\"),\n",
    "        col(\"byline.original\").alias(\"byline\")\n",
    "    )\n",
    "    \n",
    "    # 7) Add partition columns (year/month/day/hour)\n",
    "    df_part = df_sel.withColumn(\"yr\",  year(\"pub_date\")) \\\n",
    "                    .withColumn(\"mo\",  month(\"pub_date\")) \\\n",
    "                    .withColumn(\"dy\",  dayofmonth(\"pub_date\")) \\\n",
    "                    .withColumn(\"hr\",  hour(\"pub_date\"))\n",
    "    \n",
    "    # 8) Write to Delta in append mode, partitioned by (yr,mo,dy,hr)\n",
    "    df_part.write \\\n",
    "           .format(\"delta\") \\\n",
    "           .mode(\"append\") \\\n",
    "           .partitionBy(\"yr\",\"mo\",\"dy\",\"hr\") \\\n",
    "           .save(DELTA_PATH)\n",
    "           \n",
    "full_df = spark.read.format(\"delta\").load(DELTA_PATH)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
