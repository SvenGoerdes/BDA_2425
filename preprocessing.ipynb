{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75f8a93",
   "metadata": {},
   "source": [
    "## Read in Data\n",
    "\n",
    "Maybe can be removed when directly pulled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede44d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    window, first, last, max as Fmax, min as Fmin,\n",
    "    sum as Fsum, avg as Favg, stddev, lag, col,\n",
    "    split, explode, count as Fcount\n",
    ")\n",
    "from pyspark.sql.window import Window as W\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "# 1) Spark session\n",
    "spark = SparkSession.builder.appName(\"WindowedAggregation\").getOrCreate()\n",
    "\n",
    "# 2) Load raw SPY CSV (already batched pull)\n",
    "stockDF = spark.read.csv(\n",
    "    \"/mnt/project/spy_snapshot.csv\",\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "# 3) Load raw GDELT CSV (already batched pull)\n",
    "newsDF = spark.read.csv(\n",
    "    \"/mnt/project/gdelt_news.csv\",\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"timestamp\", col(\"V2_DATE\").cast(\"timestamp\")) \\\n",
    " .select(\n",
    "    \"timestamp\",\n",
    "    col(\"V1_5_TONE\").alias(\"Tone\"),\n",
    "    \"Positive\",\"Negative\",\"Polarity\",\n",
    "    \"ActivityRefDensity\",\"SelfGroupDensity\",\n",
    "    \"WordCount\",\"GKGRECORDID\",\n",
    "    \"V2_ENHANCED_THEMES\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7780f6bd",
   "metadata": {},
   "source": [
    "## Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fce7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Aggregate SPY into 15-minute windows\n",
    "stockAgg = stockDF.groupBy(window(\"timestamp\",\"15 minutes\").alias(\"w\")) \\\n",
    "    .agg(\n",
    "      first(\"open\").alias(\"open\"),\n",
    "      Fmax(\"high\").alias(\"high\"),\n",
    "      Fmin(\"low\").alias(\"low\"),\n",
    "      last(\"close\").alias(\"close\"),\n",
    "      Fsum(\"volume\").alias(\"volume\"),\n",
    "      Fsum(\"trade_count\").alias(\"trade_count\"),\n",
    "      Favg(\"vwap\").alias(\"vwap\"),\n",
    "      stddev(((col(\"high\")+col(\"low\"))/2)).alias(\"volatility\")\n",
    "    )\n",
    "\n",
    "# 5) Compute log‐return vs previous window\n",
    "winSpec = W.orderBy(\"w.start\")\n",
    "stockFeat = stockAgg \\\n",
    "    .withColumn(\"prev_close\", lag(\"close\").over(winSpec)) \\\n",
    "    .withColumn(\"log_return\", (col(\"close\")/col(\"prev_close\")).log()) \\\n",
    "    .na.fill({\"log_return\": 0.0}) \\\n",
    "    .drop(\"prev_close\")\n",
    "\n",
    "# 6) Aggregate news into 15-minute windows (numeric only)\n",
    "newsFeat = newsDF.groupBy(window(\"timestamp\",\"15 minutes\").alias(\"w\")) \\\n",
    "    .agg(\n",
    "      Fcount(\"GKGRECORDID\").alias(\"article_count\"),\n",
    "      Favg(\"Tone\").alias(\"avg_tone\"),\n",
    "      Fsum(\"Positive\").alias(\"sum_pos\"),\n",
    "      Fsum(\"Negative\").alias(\"sum_neg\"),\n",
    "      Favg(\"Polarity\").alias(\"avg_pol\")\n",
    "    )\n",
    "\n",
    "# 5) News: numeric aggregates per window\n",
    "numericNews = newsDF.groupBy(window(\"timestamp\",\"15 minutes\").alias(\"w\")) \\\n",
    "    .agg(\n",
    "      Fcount(\"GKGRECORDID\").alias(\"article_count\"),\n",
    "      Favg(\"Tone\").alias(\"avg_tone\"),\n",
    "      Fsum(\"Positive\").alias(\"sum_pos\"),\n",
    "      Fsum(\"Negative\").alias(\"sum_neg\"),\n",
    "      Favg(\"Polarity\").alias(\"avg_pol\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8097c9",
   "metadata": {},
   "source": [
    "Additional Themes feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7eceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.window import Window as W\n",
    "# from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# # 6) Prepare themes for TF–IDF\n",
    "# # explode the semicolon-delimited V1_THEMES into tokens\n",
    "# themesTokens = newsDF \\\n",
    "#   .withColumn(\"theme\", explode(split(col(\"V1_THEMES\"), \";\"))) \\\n",
    "#   .withColumn(\"theme\", col(\"theme\")) \\\n",
    "#   .groupBy(\"GKGRECORDID\",\"timestamp\") \\\n",
    "#   .agg(collect_list(\"theme\").alias(\"themes\") )\n",
    "\n",
    "# # 7) TF–IDF on themes\n",
    "# htf = HashingTF(inputCol=\"themes\", outputCol=\"tf\", numFeatures=256)\n",
    "# idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "# tf = htf.transform(themesTokens)\n",
    "# themesVec = idf.fit(tf).transform(tf)\n",
    "\n",
    "# # 8) Aggregate theme vectors per window (mean TF–IDF)\n",
    "# # use simple avg over sparse vectors: convert to array and back if needed, but Spark 3.0+ supports avg on Vector\n",
    "# themeNews = themesVec.groupBy(window(\"timestamp\",\"15 minutes\").alias(\"w\")) \\\n",
    "#     .agg( Favg(\"tfidf\").alias(\"avg_tfidf\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2600be4",
   "metadata": {},
   "source": [
    "First Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Join stock + news features\n",
    "joinedFeat = stockFeat.join(newsFeat, on=\"w\", how=\"left\") \\\n",
    "    .na.fill({\n",
    "      \"article_count\": 0,\n",
    "      \"avg_tone\": 0.0,\n",
    "      \"sum_pos\": 0.0,\n",
    "      \"sum_neg\": 0.0,\n",
    "      \"avg_pol\": 0.0,\n",
    "      #\"avg_tfidf\":  Vectors.sparse(256, [])\n",
    "    })\n",
    "\n",
    "# 8) Preview final feature table\n",
    "joinedFeat.select(\n",
    "  \"w.start\",\"w.end\",\n",
    "  \"open\",\"high\",\"low\",\"close\",\"volume\",\"trade_count\",\n",
    "  \"vwap\",\"volatility\",\"log_return\",\n",
    "  \"article_count\",\"avg_tone\",\"sum_pos\",\"sum_neg\",\"avg_pol\"#,\"avg_tfidf\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c0b3de",
   "metadata": {},
   "source": [
    "## Prepare for pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bfc5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "all_feature_cols = [\n",
    "    \"open\",\"high\",\"low\",\"close\",\n",
    "    \"volume\",\"trade_count\",\"vwap\",\n",
    "    \"volatility\",\"log_return\",\n",
    "    \"article_count\",\"avg_tone\",\"sum_pos\",\"sum_neg\",\"avg_pol\"\n",
    "]\n",
    "\n",
    "# Add depending if TF-IDF is used\n",
    "#all_feature_cols = all_feature_cols + [\"avg_tfidf\"]\n",
    "\n",
    "# 2) Create the VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=all_feature_cols,\n",
    "    outputCol=\"raw_features\"\n",
    ")\n",
    "\n",
    "# 3) Create the StandardScaler\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"raw_features\",\n",
    "    outputCol=\"features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c9d25a",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "-  Decide on model\n",
    "    - Classifictaion Binary up or down\n",
    "    - Regression\n",
    "\n",
    "- Create graphs and metrics for evaluation and visualization\n",
    "\n",
    "Optional:\n",
    "- Create feature selection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc09a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Assume `joinedFeat` from before is in scope:\n",
    "# columns: w.start, open, high, low, close, volume, trade_count, vwap,\n",
    "# volatility, log_return, article_count, avg_tone, sum_pos, sum_neg, avg_pol\n",
    "\n",
    "# 1) Build the labeled DataFrame with next-window close as label\n",
    "wSpec = Window.orderBy(\"w.start\")\n",
    "df = joinedFeat.withColumn(\"label\", lead(\"close\",1).over(wSpec)).na.drop(subset=[\"label\"])\n",
    "\n",
    "# 2) Assemble & scale features\n",
    "feature_cols = [\n",
    "    \"open\",\"high\",\"low\",\"close\",\"volume\",\"trade_count\",\"vwap\",\n",
    "    \"volatility\",\"log_return\",\"article_count\",\"avg_tone\",\"sum_pos\",\"sum_neg\",\"avg_pol\"\n",
    "]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"raw_features\")\n",
    "scaler    = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withMean=True, withStd=True)\n",
    "\n",
    "# 3) Split into train/test\n",
    "train_df, test_df = df.randomSplit([0.8,0.2], seed=42)\n",
    "\n",
    "# 4) Evaluator\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 5) GBT run\n",
    "# ----------------------------------------------------\n",
    "mlflow.spark.autolog()   # enable Spark autologging\n",
    "with mlflow.start_run(run_name=\"GBTRegressor\") as run:\n",
    "    gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"label\",\n",
    "                       maxIter=50, maxDepth=5)\n",
    "    pipeline_gbt = Pipeline(stages=[assembler, scaler, gbt])\n",
    "    model_gbt = pipeline_gbt.fit(train_df)\n",
    "    \n",
    "    preds_gbt = model_gbt.transform(test_df)\n",
    "    rmse_gbt = evaluator.evaluate(preds_gbt)\n",
    "    mlflow.log_metric(\"rmse\", rmse_gbt)\n",
    "    print(f\"GBT RMSE = {rmse_gbt:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 6) XGBoost run\n",
    "# ----------------------------------------------------\n",
    "# no need to call autolog again—the same setting applies\n",
    "with mlflow.start_run(run_name=\"XGBRegressor\") as run:\n",
    "    xgb = SparkXGBRegressor(\n",
    "        features_col=\"features\",\n",
    "        label_col=\"label\",\n",
    "        objective=\"reg:squarederror\",\n",
    "        num_round=100,\n",
    "        max_depth=5,\n",
    "        eta=0.1\n",
    "    )\n",
    "    pipeline_xgb = Pipeline(stages=[assembler, scaler, xgb])\n",
    "    model_xgb = pipeline_xgb.fit(train_df)\n",
    "    \n",
    "    preds_xgb = model_xgb.transform(test_df)\n",
    "    rmse_xgb = evaluator.evaluate(preds_xgb)\n",
    "    mlflow.log_metric(\"rmse\", rmse_xgb)\n",
    "    print(f\"XGB RMSE = {rmse_xgb:.4f}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 7) Compare in MLflow UI\n",
    "# ----------------------------------------------------\n",
    "print(\"Two runs have been logged.  Navigate to the MLflow Experiment UI to compare their RMSE and parameters side-by-side.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e98e8e",
   "metadata": {},
   "source": [
    "## Custom DCScan Algorithm for Clustering News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b78a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from math import sqrt, exp\n",
    "\n",
    "# 1.1 Extract per-article TF-IDF and window start as RDD\n",
    "# Assume themesVec has columns: window (with .start), GKGRECORDID, tfidf\n",
    "articleRDD = themesVec.rdd.map(lambda row: (\n",
    "    row['w'].start,        # window_start timestamp\n",
    "    (row['GKGRECORDID'], row['tfidf'])  # article id + vector\n",
    "))\n",
    "\n",
    "# 1.2 Group per window \n",
    "byWindow = articleRDD.groupByKey()  \n",
    "# now: RDD[(window_start, Iterable[(id, tfidf_vec)])]\n",
    "\n",
    "# 1.3 A simple density-based clusterer (DBSCAN-lite)\n",
    "def dbscan_cluster(points, eps=0.5, min_pts=3):\n",
    "    # points: list of (id, SparseVector)\n",
    "    clusters = {}\n",
    "    visited = set()\n",
    "    cid = 0\n",
    "    # naive O(n^2) distance for demo\n",
    "    def dist(u, v):\n",
    "        diff = u.toArray() - v.toArray()\n",
    "        return sqrt((diff*diff).sum())\n",
    "    for pid, vec in points:\n",
    "        if pid in visited:\n",
    "            continue\n",
    "        visited.add(pid)\n",
    "        # find neighbors\n",
    "        neigh = [q for q,w in points if dist(vec, w) <= eps]\n",
    "        if len(neigh) < min_pts:\n",
    "            clusters[pid] = -1  # noise\n",
    "        else:\n",
    "            # expand cluster\n",
    "            stack = neigh[:]\n",
    "            clusters[pid] = cid\n",
    "            while stack:\n",
    "                q = stack.pop()\n",
    "                if q not in visited:\n",
    "                    visited.add(q)\n",
    "                    qvec = dict(points)[q]\n",
    "                    qneigh = [r for r,w in points if dist(qvec, w) <= eps]\n",
    "                    if len(qneigh) >= min_pts:\n",
    "                        stack.extend(qneigh)\n",
    "                if q not in clusters:\n",
    "                    clusters[q] = cid\n",
    "            cid += 1\n",
    "    return clusters  # dict article_id → cluster_id\n",
    "\n",
    "# 1.4 Apply per window\n",
    "clustered = byWindow.mapValues(lambda articles:\n",
    "    dbscan_cluster(list(articles), eps=0.3, min_pts=2)\n",
    ")\n",
    "# clustered: RDD[(window_start, {article_id: cluster_id, …}), …]\n",
    "\n",
    "# 1.5 Turn back into a DataFrame if you want to aggregate cluster stats per window\n",
    "rows = clustered.flatMap(lambda ws_map:\n",
    "    [ (ws_map[0], aid, cid) for aid, cid in ws_map[1].items() ]\n",
    ")\n",
    "clusterDF = spark.createDataFrame(rows, schema=[\"window_start\",\"GKGID\",\"cluster_id\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
