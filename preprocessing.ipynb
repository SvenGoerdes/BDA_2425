{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d75f8a93",
   "metadata": {},
   "source": [
    "## Read in Data\n",
    "\n",
    "Maybe can be removed when directly pulled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede44d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    window, first, last, max as Fmax, min as Fmin,\n",
    "    sum as Fsum, avg as Favg, stddev, lag, col,\n",
    "    split, explode, count as Fcount\n",
    ")\n",
    "from pyspark.sql.window import Window as W\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "# 1) Spark session\n",
    "spark = SparkSession.builder.appName(\"WindowedAggregation\").getOrCreate()\n",
    "\n",
    "# 2) Load raw SPY CSV (already batched pull)\n",
    "stockDF = spark.read.csv(\n",
    "    \"/mnt/project/spy_snapshot.csv\",\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "# 3) Load raw GDELT CSV (already batched pull)\n",
    "newsDF = spark.read.csv(\n",
    "    \"/mnt/project/gdelt_news.csv\",\n",
    "    header=True, inferSchema=True\n",
    ").withColumn(\"timestamp\", col(\"V2_DATE\").cast(\"timestamp\")) \\\n",
    " .select(\n",
    "    \"timestamp\",\n",
    "    col(\"V1_5_TONE\").alias(\"Tone\"),\n",
    "    \"Positive\",\"Negative\",\"Polarity\",\n",
    "    \"ActivityRefDensity\",\"SelfGroupDensity\",\n",
    "    \"WordCount\",\"GKGRECORDID\",\n",
    "    \"V2_ENHANCED_THEMES\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7780f6bd",
   "metadata": {},
   "source": [
    "## Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fce7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Aggregate SPY into 15-minute windows\n",
    "stockAgg = stockDF.groupBy(window(\"timestamp\",\"15 minutes\").alias(\"w\")) \\\n",
    "    .agg(\n",
    "      first(\"open\").alias(\"open\"),\n",
    "      Fmax(\"high\").alias(\"high\"),\n",
    "      Fmin(\"low\").alias(\"low\"),\n",
    "      last(\"close\").alias(\"close\"),\n",
    "      Fsum(\"volume\").alias(\"volume\"),\n",
    "      Fsum(\"trade_count\").alias(\"trade_count\"),\n",
    "      Favg(\"vwap\").alias(\"vwap\"),\n",
    "      stddev(((col(\"high\")+col(\"low\"))/2)).alias(\"volatility\")\n",
    "    )\n",
    "\n",
    "# 5) Compute log‐return vs previous window\n",
    "winSpec = W.orderBy(\"w.start\")\n",
    "stockFeat = stockAgg \\\n",
    "    .withColumn(\"prev_close\", lag(\"close\").over(winSpec)) \\\n",
    "    .withColumn(\"log_return\", (col(\"close\")/col(\"prev_close\")).log()) \\\n",
    "    .na.fill({\"log_return\": 0.0}) \\\n",
    "    .drop(\"prev_close\")\n",
    "\n",
    "# 6) Aggregate news into 15-minute windows (numeric only)\n",
    "newsFeat = newsDF.groupBy(window(\"timestamp\",\"15 minutes\").alias(\"w\")) \\\n",
    "    .agg(\n",
    "      Fcount(\"GKGRECORDID\").alias(\"article_count\"),\n",
    "      Favg(\"Tone\").alias(\"avg_tone\"),\n",
    "      Fsum(\"Positive\").alias(\"sum_pos\"),\n",
    "      Fsum(\"Negative\").alias(\"sum_neg\"),\n",
    "      Favg(\"Polarity\").alias(\"avg_pol\")\n",
    "    )\n",
    "\n",
    "# 5) News: numeric aggregates per window\n",
    "numericNews = newsDF.groupBy(window(\"timestamp\",\"15 minutes\").alias(\"w\")) \\\n",
    "    .agg(\n",
    "      Fcount(\"GKGRECORDID\").alias(\"article_count\"),\n",
    "      Favg(\"Tone\").alias(\"avg_tone\"),\n",
    "      Fsum(\"Positive\").alias(\"sum_pos\"),\n",
    "      Fsum(\"Negative\").alias(\"sum_neg\"),\n",
    "      Favg(\"Polarity\").alias(\"avg_pol\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8097c9",
   "metadata": {},
   "source": [
    "Additional Themes feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7eceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.window import Window as W\n",
    "# from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# # 6) Prepare themes for TF–IDF\n",
    "# # explode the semicolon-delimited V1_THEMES into tokens\n",
    "# themesTokens = newsDF \\\n",
    "#   .withColumn(\"theme\", explode(split(col(\"V1_THEMES\"), \";\"))) \\\n",
    "#   .withColumn(\"theme\", col(\"theme\")) \\\n",
    "#   .groupBy(\"GKGRECORDID\",\"timestamp\") \\\n",
    "#   .agg(collect_list(\"theme\").alias(\"themes\") )\n",
    "\n",
    "# # 7) TF–IDF on themes\n",
    "# htf = HashingTF(inputCol=\"themes\", outputCol=\"tf\", numFeatures=256)\n",
    "# idf = IDF(inputCol=\"tf\", outputCol=\"tfidf\")\n",
    "# tf = htf.transform(themesTokens)\n",
    "# themesVec = idf.fit(tf).transform(tf)\n",
    "\n",
    "# # 8) Aggregate theme vectors per window (mean TF–IDF)\n",
    "# # use simple avg over sparse vectors: convert to array and back if needed, but Spark 3.0+ supports avg on Vector\n",
    "# themeNews = themesVec.groupBy(window(\"timestamp\",\"15 minutes\").alias(\"w\")) \\\n",
    "#     .agg( Favg(\"tfidf\").alias(\"avg_tfidf\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2600be4",
   "metadata": {},
   "source": [
    "First Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1df3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Join stock + news features\n",
    "joinedFeat = stockFeat.join(newsFeat, on=\"w\", how=\"left\") \\\n",
    "    .na.fill({\n",
    "      \"article_count\": 0,\n",
    "      \"avg_tone\": 0.0,\n",
    "      \"sum_pos\": 0.0,\n",
    "      \"sum_neg\": 0.0,\n",
    "      \"avg_pol\": 0.0,\n",
    "      #\"avg_tfidf\":  Vectors.sparse(256, [])\n",
    "    })\n",
    "\n",
    "# 8) Preview final feature table\n",
    "joinedFeat.select(\n",
    "  \"w.start\",\"w.end\",\n",
    "  \"open\",\"high\",\"low\",\"close\",\"volume\",\"trade_count\",\n",
    "  \"vwap\",\"volatility\",\"log_return\",\n",
    "  \"article_count\",\"avg_tone\",\"sum_pos\",\"sum_neg\",\"avg_pol\"#,\"avg_tfidf\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c0b3de",
   "metadata": {},
   "source": [
    "## Prepare for pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bfc5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "all_feature_cols = [\n",
    "    \"open\",\"high\",\"low\",\"close\",\n",
    "    \"volume\",\"trade_count\",\"vwap\",\n",
    "    \"volatility\",\"log_return\",\n",
    "    \"article_count\",\"avg_tone\",\"sum_pos\",\"sum_neg\",\"avg_pol\"\n",
    "]\n",
    "\n",
    "# Add depending if TF-IDF is used\n",
    "#all_feature_cols = all_feature_cols + [\"avg_tfidf\"]\n",
    "\n",
    "# 2) Create the VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=all_feature_cols,\n",
    "    outputCol=\"raw_features\"\n",
    ")\n",
    "\n",
    "# 3) Create the StandardScaler\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"raw_features\",\n",
    "    outputCol=\"features\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c9d25a",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "-  Decide on model\n",
    "    - Classifictaion Binary up or down\n",
    "    - Regression\n",
    "\n",
    "- Create graphs and metrics for evaluation and visualization\n",
    "\n",
    "Optional:\n",
    "- Create feature selection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc09a3a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2dd609b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7779d522",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4db3c2b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a74f1ebe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
